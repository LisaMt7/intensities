<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <title>Intensity | Audio Visualization</title>
  <!-- <script src="https://cdn.jsdelivr.net/npm/brainsatplay-components"></script> -->
</head>
<!-- <script src="dist/app.js" defer=true></script> -->

<style>

* {
  box-sizing: border-box;
}

  html, body {
    margin: 0px;
    font-family: sans-serif;
    overflow: hidden;
  }

  #overlay {
    opacity: 0;
    width: 100vw;
    height: 100vh;
    transition: 0.5s;
    position: fixed;
    top: 0;
    left: 0;
    pointer-events: none;
    display: flex;
    align-items: center;
    justify-content: center;
    z-index: 1;
    font-size: 30px;
    font-weight: bold;
    /* background: black; */
  }

  #overlay.open {
    opacity: 1;
    pointer-events: all;
    backdrop-filter: blur(5px);
  }

  .disabled {
    pointer-events: none;
    opacity: 0.5;
  }

  #sidebar select, #sidebar input {
    width: 200px;
  }

  body {
    display: grid;
    grid-template-columns: 1fr 1fr 300px;
    grid-template-rows: 75px 1fr 75px;
    grid-template-areas: 
            "nav nav nav"
            "main main side"
            "foot foot side";

    width: 100vw;
    height: 100vh;
  }

  nav, footer {
    z-index: 2;
    padding: 20px;
    height: 75px;
    background: black;
    color: white;
    font-weight: bold;
    display:flex;
    align-items: center;
    /* justify-content: center; */
  }

  nav {
    grid-area: nav;
  }

  section {
    padding: 25px;
  }

  video {
    width: 100%;
  }

  footer {
    position: fixed;
    bottom: 0;
    left: 0;
    width: 100%;
    font-size: 70%;
    font-weight: normal;
    grid-area: footer;
  }

  #main {
    grid-area: main;
    display: flex;
    flex-wrap: wrap;
    overflow: scroll;
  }
  
  #main > div {
    max-width: 400px;
  }

  #sidebar {
    background: rgb(234, 234, 234);
    padding: 25px;
    grid-area: auto;
    grid-area: side;
    height: 100%;
    width: 100%;
    /* z-index:1; */
  }

</style>

<body>
  <nav>Intensity</nav>
  <div id="overlay">Loading file...</div>

  <!-- Interactive Sidebar -->
  <div id="sidebar">
    <h3>Controls</h3>
    <h4>Real-Time Data</h4>
    <button id="start">Start Data Acquisition</button>

    <h5>Audio Input</h5>
    <select id="in"></select>

    <h5>Audio Output</h5>
    <select id="out"></select>

    <h5>Video Input</h5>
    <select id="video"></select>

    <h4>Upload Files</h4>
    <fieldset>
      <input type="file" id="files" accept="audio/*, video/*" />
    </fieldset>
  </div>

  <!-- Main Visualization Area -->
  <section id="main">
  </section>

  <footer>Garrett Flynn and Lisa MÃ¼ller-Trede | 2022</footer>
</body>

<script type="module">

  // import * as components from "./dist/index.esm.js"
  import * as components from "https://cdn.jsdelivr.net/npm/brainsatplay-components@latest/dist/index.esm.js"

  // Bypass the usual requirement for user action
  const start = document.getElementById('start')
  const audioInputSelect = document.getElementById('in')
  const audioOutputSelect = document.getElementById('out')
  const videoSelect = document.getElementById('video')
  var fileInput = document.getElementById('files');
  var main = document.getElementById('main');
  var overlay = document.getElementById('overlay');

  navigator.mediaDevices.enumerateDevices()
    .then(gotDevices)
  // .catch(errorCallback);


  let context = null; // Initialize on user action
  let analyser = null;
  let filterNode = null;
  let gainNode = null;

  const initializeContext = () => {
    if (!context) {
      setInterval(bigAnalysisLoop, 50); // Get Data Every 100ms
      context = new (window.AudioContext || window.webkitAudioContext)();
    // Setup Analysis
      analyser = context.createAnalyser();
      analyser.smoothingTimeConstant = 0.2;
      analyser.fftSize = 1024;
      analyser.minDecibels = -127;
      analyser.maxDecibels = 0;

      filterNode = context.createBiquadFilter();
      filterNode.type = 'highpass';
      filterNode.frequency.value = 7000;

      gainNode = context.createGain(); // Create a gain node to change audio volume.
      gainNode.gain.value = 1.0;

        
      filterNode.connect(gainNode);
      // microphone.connect(gainNode);
      gainNode.connect(analyser);
      // analyser.connect(context.destination);

        // var source = context.createBufferSource();
      // source.buffer = data;
    }

  }

  let self = false

  const sourceRegistry = {}

  // video.controls = true

  function gotDevices(deviceInfos) {
    for (var i = 0; i !== deviceInfos.length; ++i) {
      var deviceInfo = deviceInfos[i];
      var option = document.createElement('option');
      option.value = deviceInfo.deviceId;
      if (deviceInfo.kind === 'audioinput') {
        option.text = deviceInfo.label ||
          'Microphone ' + (audioInputSelect.length + 1);
        audioInputSelect.appendChild(option);
      } else if (deviceInfo.kind === 'audiooutput') {
        option.text = deviceInfo.label || 'Speaker ' +
          (audioOutputSelect.length + 1);
        audioOutputSelect.appendChild(option);
      } else if (deviceInfo.kind === 'videoinput') {
        option.text = deviceInfo.label || 'Camera ' +
          (videoSelect.length + 1);
        videoSelect.appendChild(option);
      }
    }
  }

  const analyses = {}
  const integrations = {}
  

  const createAnalyser = () => {
    const analyser = context.createAnalyser();
    analyser.smoothingTimeConstant = 0.2;
    analyser.fftSize = 1024;
    analyser.minDecibels = -127;
    analyser.maxDecibels = 0;
    return analyser
  }

  const analyse = (o, i) => {
      // Analyze the Data
      let volumeCallback = null;
      let volumeInterval = null;
      const frequencies = new Uint8Array(o.analyser.frequencyBinCount);
      // let raw = new Uint8Array(1) // Only get the latest
      const getData = () => {
        o.analyser.getByteFrequencyData(frequencies);
        // o.analyser.getByteTimeDomainData(raw)
        // const arr = Array.from(raw)
        const freqArr = Array.from(frequencies)
        // o.timeseries.data = [arr]
        o.spectrogram.data = freqArr
        return {
          // timeseries: arr, 
          frequencies: freqArr
        }
      };

      analyses[i] = {
        function: getData,
        output: null
      }
  }

  const integrate = (key, o, iArr, integrator = (arr) => {}) => {

      integrations[key] = {
        function: () => {
          const o2 = integrator(iArr.map(i => analyses[i].output))
          // o.timeseries.data = [o2.timeseries]
          o.spectrogram.data = o2.frequencies
        },
        output: null
      }
  }


  const bigAnalysisLoop = () => {

    // First Analyses
    for (let k in analyses) {analyses[k].output = analyses[k].function()}

    // Then Integrations
    for (let k in integrations) {integrations[k].output =  integrations[k].function()}
  }

  const onStreamSource = (src, inputs={}) => {

      src.connect(filterNode); // Connect to top of Web Audio API Context

      // TODO: Split tracks from video
      const channels = src.numberOfChannels ?? src.buffer?.numberOfChannels
      if (channels > 1) {
        var splitter = context.createChannelSplitter(channels);
        analyser.connect(splitter); // Connect end of main graph to the Splitter graph
        var merger = context.createChannelMerger(channels);

        for (let i = 0; i < channels; i++){
          const o = spawnStreamDisplay(count, inputs) // Create Display
          o.container.insertAdjacentHTML('afterbegin',`<h3>Channel ${i}</h3>`)
          const gainNode = context.createGain();
          gainNode.gain.setValueAtTime(1.0, context.currentTime);
          o.analyser = createAnalyser()
          splitter.connect(o.analyser, i) // Connect Analyser to Channel i
          o.analyser.connect(gainNode);
          gainNode.connect(merger, 0, i) // Merge split inputs
          analyse(o, count) // Start analysis
          count++
        }

        // Similarity (first two)
        const o = spawnStreamDisplay(count, inputs) // Create Display
        o.container.insertAdjacentHTML('afterbegin',`<h3>Similarity</h3>`)

        // const maxHist = 1000
        // const averageDifference = {
        //   // timeseries: [], 
        //   frequencies: []
        // }

        const average = (arr) => arr.reduce((a,b) => a + b, 0) / arr.length
        // let start = Date.now()
        // let compareCount = 0
        const similarityNorm = (arr, max, key) => {

          // compareCount++

          const res = arr.map(v => {
            return Math.abs(1-v/max)
            // const similarity = 1 - v/max
            // averageDifference[key].push(similarity)
            // if (averageDifference[key].length > maxHist) averageDifference[key].shift()
            // return Math.abs(similarity - average(averageDifference[key]))
          })

          // if (compareCount >= maxHist) { 
          //     compareCount = 0
          //     const done = Date.now()
          //     console.log(`${(done - start)/1000}s rolling average`)
          //     start = done
          //   }

            return res
        }

        const difference = (arr1, arr2, key) => {
            return arr1.map((v, i) => {
                const diff = v - arr2[i]
                // averageDifference[key].push(diff)
                // if (averageDifference[key].length > maxHist) averageDifference[key].shift()
                // return Math.abs(diff - average(averageDifference[key]))
                return diff
            })
          }

        integrate('similarity', o, [0, 1], (arr) => {
          const o = {}
          // const differenceTime = difference(arr[0].timeseries, arr[1].timeseries, 'timeseries')
          const differenceFreq = difference(arr[0].frequencies, arr[1].frequencies, 'frequencies')
          const maxFreqDiff = Math.max(...differenceFreq)
          // const maxTimeDiff = Math.max(...differenceTime)
          // console.log(maxFreqDiff, maxTimeDiff)

          // o.timeseries = differenceTime.map(v => 1 - v/maxTimeDiff) // Difference
          const similarityFreq = similarityNorm(differenceFreq, maxFreqDiff, 'frequencies') // Similarity
          o.frequencies = similarityFreq
          return o
        }) // Start integration
        count++

        // Additive (first two)
        const o2 = spawnStreamDisplay(count, inputs) // Create Display
        o2.container.insertAdjacentHTML('afterbegin',`<h3>Additive</h3>`)
        const add = (arr1, arr2) => {
            return arr1.map((v, i) => {
                return v + arr2[i]
            })
          }

        integrate('additive', o2, [0, 1], (arr) => {
          const o = {}
          const addedFreqs = add(arr[0].frequencies, arr[1].frequencies)
          o.frequencies = addedFreqs
          return o
        })
        count++

        // Remove common (first two)
        const o3 = spawnStreamDisplay(count, inputs) // Create Display
        o3.container.insertAdjacentHTML('afterbegin',`<h3>Remove Common Noise</h3>`)
        const subCommon = (arr1, arr2) => {
            return arr1.map((v, i) => {
                return Math.abs(v - arr2[i])
            })
          }

        integrate('common', o3, [0, 1], (arr) => {
          const o = {}
          const addedFreqs = subCommon(arr[0].frequencies, arr[1].frequencies)
          o.frequencies = addedFreqs
          return o
        })
        count++


        if (!self) merger.connect(context.destination); // Output to the speakers
      } 
      else {
        const o = spawnStreamDisplay(count, inputs) // Create Display
        o.analyser = analyser
        if (!self) analyser.connect(context.destination)
        analyse(o, count) // Start analysis
      }

      if (video){
        video.onended = () => {
          src.disconnect();
          gainNode.disconnect();
          filterNode.disconnect()
        }
      }

      if (src.start instanceof Function) src.start()
  }

  const spawnStreamDisplay = (count, o = {}) => {
        const container = document.createElement('div')
        // if (!o.video) o.video = document.createElement('video')
        main.insertAdjacentElement('beforeend', container)

        if (o.video){
          container.insertAdjacentElement('beforeend', o.video)
          // Real-Time Input
          if (o.stream) {
            o.video.srcObject = o.stream
            o.video.controls = false
          } 
          else {
            o.video.controls = true
          }
          o.video.autoplay = true
        }

        
        sourceRegistry[count] = {
            container,
            video: o.video,
            stream: o.stream,
            spectrogram: new components.streams.data.Spectrogram(),
            // timeseries: new components.streams.data.TimeSeries(),
        }

        container.insertAdjacentElement('beforeend', sourceRegistry[count].spectrogram)
        // container.insertAdjacentElement('beforeend', sourceRegistry[count].timeseries)
        return sourceRegistry[count]
    }


  let count = 0

  fileInput.onchange = async (ev) => {
    initializeContext()
    count = 0 // Reset count with new file...
    for (let file of fileInput.files) {
      const type = file.type.split('/')[0]
      let source, video;

      if (type === 'video'){
          video = document.createElement('video')
          video.src = URL.createObjectURL(file)
          source = context.createMediaElementSource(video);
      } else {
        source = await onAudio(file);
      }

      onStreamSource(source, {video}) // Get Audio Features + Wire Audio Analysis + Create Display
      // count added in here
    }

    main.style.gridTemplateColumns = `repeat(${count},1fr)`
  }

  start.onclick = () => {

    initializeContext()

    self = true
    navigator.mediaDevices.getUserMedia({
      audio: {
        deviceId: { exact: audioInputSelect.value }
      }, 
      video: { exact: videoSelect.value }
    }).then((stream) => {
      const video = document.createElement('video')
      const microphone = context.createMediaStreamSource(stream);
      onStreamSource(microphone, {video, stream})
    })
  }


  // var reader1 = new FileReader();

  const onAudio =(file) => {
  return new Promise((resolve, reject) => {
    let reader = new FileReader();

    reader.onload = (ev) => {

      overlay.classList.add('open')
      context.decodeAudioData(ev.target.result, (data) => {
        overlay.classList.remove('open')
        const source = context.createBufferSource();
        source.buffer = data;
        resolve(source);
      })
    };

    function handleEvent(event) {
    console.log(`${event.type}: ${event.loaded} bytes transferred\n`)

    if (event.type === "load") {
        // preview.src = reader.result;
    }
}

    // reader.addEventListener('loadstart', handleEvent);
    // reader.addEventListener('load', handleEvent);
    // reader.addEventListener('loadend', handleEvent);
    // reader.addEventListener('progress', handleEvent);
    reader.addEventListener('error', handleEvent);
    // reader.addEventListener('abort', handleEvent);

    reader.readAsArrayBuffer(file);
  })
}

</script>

</html>