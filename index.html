<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <title>Audio Visualization</title>
  <!-- <script src="https://cdn.jsdelivr.net/npm/brainsatplay-components"></script> -->
</head>

<style>
  video {
    width: 300px;
  }

</style>

<body>
  <button id="start">Start</button>
  <select id="in"></select>
  <select id="out"></select>
  <select id="video"></select>

  <h5>Upload Files (one audio or video)</h5>
  <fieldset>
    <input type="file" id="files" accept="audio/*, video/*" />
    <!-- <button type="button" id="compress_btn" class="btn btn-outline-primary" style="margin-top: 20px;">Compress</button> -->
  </fieldset>

  <video></video>
</body>

<script type="module">

  // import * as components from "./dist/index.esm.js"
  import * as components from "https://cdn.jsdelivr.net/npm/brainsatplay-components@latest/dist/index.esm.js"

  // Bypass the usual requirement for user action
  const start = document.getElementById('start')
  const audioInputSelect = document.getElementById('in')
  const audioOutputSelect = document.getElementById('out')
  const videoSelect = document.getElementById('video')
  var fileInput = document.getElementById('files');

  var video = document.querySelector('video');

  const spectrogram = new components.streams.data.Spectrogram()
  const timeseries = new components.streams.data.TimeSeries()
  document.body.insertAdjacentElement('beforeend', spectrogram)
  document.body.insertAdjacentElement('beforeend', timeseries)

  navigator.mediaDevices.enumerateDevices()
    .then(gotDevices)
  // .catch(errorCallback);


  const context = new AudioContext();
  video.controls = true

  function gotDevices(deviceInfos) {
    console.log(deviceInfos)
    for (var i = 0; i !== deviceInfos.length; ++i) {
      var deviceInfo = deviceInfos[i];
      var option = document.createElement('option');
      option.value = deviceInfo.deviceId;
      if (deviceInfo.kind === 'audioinput') {
        option.text = deviceInfo.label ||
          'Microphone ' + (audioInputSelect.length + 1);
        audioInputSelect.appendChild(option);
      } else if (deviceInfo.kind === 'audiooutput') {
        option.text = deviceInfo.label || 'Speaker ' +
          (audioOutputSelect.length + 1);
        audioOutputSelect.appendChild(option);
      } else if (deviceInfo.kind === 'videoinput') {
        option.text = deviceInfo.label || 'Camera ' +
          (videoSelect.length + 1);
        videoSelect.appendChild(option);
      }
    }
  }
  const onFreqs = (frequencies) => {
    spectrogram.data = Array.from(frequencies)
  }

  const onAudio = (src) => {
    reader1.readAsArrayBuffer(src);
  }

  const onStreamSource = (src, stream) => {
      src.connect(filterNode);

      stream.onended = () => {
        src.disconnect();
        gainNode.disconnect();
        filterNode.disconnect()
      }

      // Show Audio Volume
      let volumeCallback = null;
      let volumeInterval = null;
      const frequencies = new Uint8Array(analyser.frequencyBinCount);
      let raw = new Uint8Array(1) // Only get the latest
      const getData = () => {
        analyser.getByteFrequencyData(frequencies);
        analyser.getByteTimeDomainData(raw)
        const arr = Array.from(raw)
        timeseries.data = [arr]
        onFreqs(frequencies)
      };

      setInterval(getData, 100); // Get Data Every 100ms
  }

  var reader1 = new FileReader();

  fileInput.onchange = (ev) => {

    // TODO: Check if audio or video
    // If video, then get the audio
    for (let file of fileInput.files) {
      switch (file.type.split('/')[0]) {
        case 'video':
          video.src = URL.createObjectURL(file)
          var source = context.createMediaElementSource(video);
          // video.autoplay = true
          onStreamSource(source, video)
          break;
        default:
          onAudio(file);
          break;
      }
    }
  }

  // Setup Analysis
  var analyser = context.createAnalyser();
      analyser.smoothingTimeConstant = 0.2;
      analyser.fftSize = 1024;
      analyser.minDecibels = -127;
      analyser.maxDecibels = 0;

      var filterNode = context.createBiquadFilter();
      filterNode.type = 'highpass';
      filterNode.frequency.value = 7000;

      var gainNode = context.createGain(); // Create a gain node to change audio volume.
      gainNode.gain.value = 1.0;

      
  filterNode.connect(gainNode);
      // microphone.connect(gainNode);
      gainNode.connect(analyser);
      analyser.connect(context.destination);

  start.onclick = () => {

    console.log('START')

    navigator.mediaDevices.getUserMedia({
      audio: {
        deviceId: { exact: audioInputSelect.value }
      }
    }).then((stream) => {
      const microphone = context.createMediaStreamSource(stream);
      onStreamSource(microphone, stream)
    })
  }

  reader1.onload = (ev) => {

    context.decodeAudioData(ev.target.result, (data) => {

      var source = context.createBufferSource();
      source.buffer = data;
      var splitter = context.createChannelSplitter(2);
      source.connect(splitter);
      var merger = context.createChannelMerger(2);

      // Reduce the volume of the left channel only
      var gainNode = context.createGain();
      gainNode.gain.setValueAtTime(0.5, context.currentTime);
      splitter.connect(gainNode, 0);

      // Connect the splitter back to the second input of the merger: we
      // effectively swap the channels, here, reversing the stereo image.
      gainNode.connect(merger, 0, 1);
      splitter.connect(merger, 1, 0);

      // merger.connect(this.out)
      merger.connect(context.destination);
    })
  }

</script>

</html>